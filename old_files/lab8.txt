Paper 1: S14-2009
This paper describes SemEval Task 9. The purpose of the task was to promote 
research into better understanding how sentiment is conveyed in social media. 
The task was comprised of two subtasks, A and B. The subtask that the majority
of teams worked on (including ourselves) was subtask B, which involved tagging 
tweets or other text-based forms of social media based on sentiment. The goal
was to use a corpus of example tweets as training data and design an algorithm
that could use that information to correct tag a set of test data. All of the 
participants beat the MFS baseline, but there was quite a range between the 
best and the worst teams. There were also subtasks to this subtask, which 
included LiveJournal data and sarcastic tweets. The sarcastic tweets in 
particular tended to be more difficult to classify than the standard tweets.

Paper 2: S14-2111
This paper descibes a solution to task B implemented by TeamX from Japan. 
The main features that separated their implementation from our own are that 
they had a few extra feature extraction/pre-processing methods (which include: 
a spelling corrector, two POS taggers, and a word sense disambiguator, words/
ngrams/non-contiguous ngrams) and the fact that they used a different machine
learning method. While we used Naive Bayes, they decided to use Logistic 
Regression, specifically LIBLINEAR. They scored very well on most of the 
datasets,though they did do relatively poorly (compared to their other scores)
on the Twitter Sarcasm task. 

Paper 3: S14-2048
This paper also describes a solution to task B. This implementation involved
using a support vector machine (SVM) as their machine learning algorithm. 
Their preprocessing was pretty similar to ours. The main addition 
to their solution was the use of ngrams and non-contiguous ngrams, which they
called skipgrams. In general, they scored quite poorly, about 15% lower than 
the previous paper. The notable exception is the sarcasm case, where they did 
quite well, slightly outperforming the first paper (53.32 vs. 53.9). This 
raises the question as to whether there is some feature of this implementation 
that was particularly effective with regard to sarcastic tweets that, when 
combined with a superior implementation, could improve other teams' sarcasm 
scores. 
